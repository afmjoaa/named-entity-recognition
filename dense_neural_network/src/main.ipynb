{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pre_process import PreProcess\n",
    "from word_to_vector import WordToVector\n",
    "from one_hot_encoding import OneHotEncoder\n",
    "import numpy as np\n",
    "from training import DnnTraining\n",
    "from inference import DnnInference\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186433 53266 26635\n",
      "186433 53266 26635\n"
     ]
    }
   ],
   "source": [
    "word_arr_one, label_arr_one = PreProcess.getTrainingTuple(dataFile='../../data/en-train.conll', onlyBioTagging=True)\n",
    "word_arr_two, label_arr_two = PreProcess.getTrainingTuple(dataFile='../../data/en-dev.conll', onlyBioTagging=True)\n",
    "\n",
    "word_arr = word_arr_one + word_arr_two\n",
    "label_arr = label_arr_one + label_arr_two\n",
    "\n",
    "n = len(word_arr)\n",
    "train_split = int(0.7 * n)\n",
    "val_split = int(0.2 * n)\n",
    "train_word_arr = word_arr[:train_split]\n",
    "val_word_arr = word_arr[train_split:train_split + val_split]\n",
    "test_word_arr = word_arr[train_split + val_split:]\n",
    "print(len(train_word_arr), len(val_word_arr), len(test_word_arr))\n",
    "\n",
    "m = len(label_arr)\n",
    "train_label_split = int(0.7 * m)\n",
    "val_label_split = int(0.2 * m)\n",
    "train_label_arr = label_arr[:train_label_split]\n",
    "val_label_arr = label_arr[train_label_split:train_label_split + val_label_split]\n",
    "test_label_arr = label_arr[train_label_split + val_label_split:]\n",
    "print(len(train_label_arr), len(val_label_arr), len(test_label_arr))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5827/5827 [==============================] - 5s 823us/step - loss: 0.4312 - accuracy: 0.8373 - f1_score: 0.8347 - val_loss: 0.4507 - val_accuracy: 0.8299 - val_f1_score: 0.8240\n",
      "Epoch 2/100\n",
      "5827/5827 [==============================] - 5s 816us/step - loss: 0.3916 - accuracy: 0.8507 - f1_score: 0.8481 - val_loss: 0.4410 - val_accuracy: 0.8353 - val_f1_score: 0.8298\n",
      "Epoch 3/100\n",
      "5827/5827 [==============================] - 5s 812us/step - loss: 0.3849 - accuracy: 0.8531 - f1_score: 0.8503 - val_loss: 0.4368 - val_accuracy: 0.8344 - val_f1_score: 0.8307\n",
      "Epoch 4/100\n",
      "5827/5827 [==============================] - 5s 811us/step - loss: 0.3796 - accuracy: 0.8547 - f1_score: 0.8521 - val_loss: 0.4380 - val_accuracy: 0.8362 - val_f1_score: 0.8325\n",
      "Epoch 5/100\n",
      "5827/5827 [==============================] - 5s 805us/step - loss: 0.3760 - accuracy: 0.8552 - f1_score: 0.8536 - val_loss: 0.4309 - val_accuracy: 0.8371 - val_f1_score: 0.8314\n",
      "Epoch 6/100\n",
      "5827/5827 [==============================] - 5s 813us/step - loss: 0.3734 - accuracy: 0.8562 - f1_score: 0.8545 - val_loss: 0.4326 - val_accuracy: 0.8357 - val_f1_score: 0.8300\n",
      "Epoch 7/100\n",
      "5827/5827 [==============================] - 5s 815us/step - loss: 0.3713 - accuracy: 0.8566 - f1_score: 0.8552 - val_loss: 0.4383 - val_accuracy: 0.8357 - val_f1_score: 0.8344\n",
      "Epoch 8/100\n",
      "5827/5827 [==============================] - 5s 816us/step - loss: 0.3691 - accuracy: 0.8581 - f1_score: 0.8562 - val_loss: 0.4340 - val_accuracy: 0.8352 - val_f1_score: 0.8319\n"
     ]
    }
   ],
   "source": [
    "# Get wordToVector from [wordArr] and oneHotEncoding from [labelArr]\n",
    "wordToVecArr = WordToVector.getPretrainedWordToVecList(train_word_arr)\n",
    "oneHotEncodingArr = OneHotEncoder.getOneHotEncodingOfOutput(train_label_arr)\n",
    "# Convert python array to num py array\n",
    "np_wordToVecArr = np.array(wordToVecArr)\n",
    "np_oneHotEncodingArr = np.array(oneHotEncodingArr)\n",
    "\n",
    "# Get wordToVector from [wordArr] and oneHotEncoding from [labelArr]\n",
    "val_wordToVecArr = WordToVector.getPretrainedWordToVecList(val_word_arr)\n",
    "val_oneHotEncodingArr = OneHotEncoder.getOneHotEncodingOfOutput(val_label_arr)\n",
    "# Convert python array to num py array\n",
    "val_np_wordToVecArr = np.array(val_wordToVecArr)\n",
    "val_np_oneHotEncodingArr = np.array(val_oneHotEncodingArr)\n",
    "\n",
    "training = DnnTraining(input_dim=300, output_dim=3)\n",
    "training.startTraining(np_wordToVecArr, np_oneHotEncodingArr, val_np_wordToVecArr, val_np_oneHotEncodingArr, epochs=100)\n",
    "training.saveTrainedModel()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833/833 [==============================] - 0s 395us/step\n",
      "Accuracy: 0.8422376572179463\n",
      "Precision: 0.6687599810866467\n",
      "Recall: 0.5534568438305022\n",
      "F1 score: 0.5921776219501038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = training.getCurrentModel()\n",
    "\n",
    "# Get wordToVector from [TestWordArr] and oneHotEncoding from [TestLabelArr]\n",
    "test_wordToVecArr = WordToVector.getPretrainedWordToVecList(test_word_arr)\n",
    "test_oneHotEncodingArr = OneHotEncoder.getOneHotEncodingOfOutput(test_label_arr)\n",
    "# Convert python array to num py array\n",
    "test_np_wordToVecArr = np.array(test_wordToVecArr)\n",
    "test_np_oneHotEncodingArr = np.array(test_oneHotEncodingArr)\n",
    "\n",
    "# Assuming you have test data and labels\n",
    "x_test = test_np_wordToVecArr\n",
    "y_test = test_np_oneHotEncodingArr\n",
    "\n",
    "# Perform inference on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_test_argmax = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class labels (if needed)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test_argmax, y_pred_classes)\n",
    "precision = precision_score(y_test_argmax, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_test_argmax, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_test_argmax, y_pred_classes, average='macro')\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
