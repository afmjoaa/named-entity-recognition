{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pre_process import PreProcess\n",
    "from word_to_vector import WordToVector\n",
    "from one_hot_encoding import OneHotEncoder\n",
    "import numpy as np\n",
    "from training import DnnTraining\n",
    "from inference import DnnInference\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186433 53266 26635\n",
      "186433 53266 26635\n"
     ]
    }
   ],
   "source": [
    "word_arr_one, label_arr_one = PreProcess.getTrainingTuple(dataFile='../../data/en-train.conll', onlyBioTagging=True)\n",
    "word_arr_two, label_arr_two = PreProcess.getTrainingTuple(dataFile='../../data/en-dev.conll', onlyBioTagging=True)\n",
    "\n",
    "word_arr = word_arr_one + word_arr_two\n",
    "label_arr = label_arr_one + label_arr_two\n",
    "\n",
    "n = len(word_arr)\n",
    "train_split = int(0.7 * n)\n",
    "val_split = int(0.2 * n)\n",
    "train_word_arr = word_arr[:train_split]\n",
    "val_word_arr = word_arr[train_split:train_split + val_split]\n",
    "test_word_arr = word_arr[train_split + val_split:]\n",
    "print(len(train_word_arr), len(val_word_arr), len(test_word_arr))\n",
    "\n",
    "m = len(label_arr)\n",
    "train_label_split = int(0.7 * m)\n",
    "val_label_split = int(0.2 * m)\n",
    "train_label_arr = label_arr[:train_label_split]\n",
    "val_label_arr = label_arr[train_label_split:train_label_split + val_label_split]\n",
    "test_label_arr = label_arr[train_label_split + val_label_split:]\n",
    "print(len(train_label_arr), len(val_label_arr), len(test_label_arr))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set preprocess done\n",
      "Validation set preprocess done\n",
      "Epoch 1/100\n",
      "5827/5827 [==============================] - 5s 817us/step - loss: 0.4332 - accuracy: 0.8391 - f1_score: 0.8354 - val_loss: 0.4436 - val_accuracy: 0.8327 - val_f1_score: 0.8249\n",
      "Epoch 2/100\n",
      "5827/5827 [==============================] - 5s 789us/step - loss: 0.3925 - accuracy: 0.8497 - f1_score: 0.8468 - val_loss: 0.4385 - val_accuracy: 0.8353 - val_f1_score: 0.8288\n",
      "Epoch 3/100\n",
      "5827/5827 [==============================] - 5s 793us/step - loss: 0.3844 - accuracy: 0.8520 - f1_score: 0.8503 - val_loss: 0.4344 - val_accuracy: 0.8349 - val_f1_score: 0.8310\n",
      "Epoch 4/100\n",
      "5827/5827 [==============================] - 5s 815us/step - loss: 0.3813 - accuracy: 0.8545 - f1_score: 0.8514 - val_loss: 0.4325 - val_accuracy: 0.8348 - val_f1_score: 0.8306\n",
      "Epoch 5/100\n",
      "5827/5827 [==============================] - 5s 776us/step - loss: 0.3766 - accuracy: 0.8552 - f1_score: 0.8532 - val_loss: 0.4347 - val_accuracy: 0.8356 - val_f1_score: 0.8309\n",
      "Epoch 6/100\n",
      "5827/5827 [==============================] - 5s 797us/step - loss: 0.3746 - accuracy: 0.8560 - f1_score: 0.8537 - val_loss: 0.4361 - val_accuracy: 0.8338 - val_f1_score: 0.8306\n",
      "Epoch 7/100\n",
      "5827/5827 [==============================] - 5s 787us/step - loss: 0.3720 - accuracy: 0.8567 - f1_score: 0.8548 - val_loss: 0.4478 - val_accuracy: 0.8344 - val_f1_score: 0.8304\n",
      "Epoch 8/100\n",
      "5827/5827 [==============================] - 5s 845us/step - loss: 0.3697 - accuracy: 0.8581 - f1_score: 0.8557 - val_loss: 0.4383 - val_accuracy: 0.8345 - val_f1_score: 0.8321\n",
      "Epoch 9/100\n",
      "5827/5827 [==============================] - 5s 828us/step - loss: 0.3690 - accuracy: 0.8576 - f1_score: 0.8559 - val_loss: 0.4463 - val_accuracy: 0.8349 - val_f1_score: 0.8320\n",
      "Epoch 10/100\n",
      "5827/5827 [==============================] - 5s 825us/step - loss: 0.3668 - accuracy: 0.8577 - f1_score: 0.8566 - val_loss: 0.4390 - val_accuracy: 0.8368 - val_f1_score: 0.8337\n",
      "Epoch 11/100\n",
      "5827/5827 [==============================] - 5s 829us/step - loss: 0.3652 - accuracy: 0.8581 - f1_score: 0.8569 - val_loss: 0.4397 - val_accuracy: 0.8353 - val_f1_score: 0.8317\n",
      "Epoch 12/100\n",
      "5827/5827 [==============================] - 5s 821us/step - loss: 0.3646 - accuracy: 0.8585 - f1_score: 0.8570 - val_loss: 0.4409 - val_accuracy: 0.8364 - val_f1_score: 0.8326\n",
      "Epoch 13/100\n",
      "5827/5827 [==============================] - 5s 797us/step - loss: 0.3631 - accuracy: 0.8593 - f1_score: 0.8578 - val_loss: 0.4576 - val_accuracy: 0.8381 - val_f1_score: 0.8357\n",
      "Epoch 14/100\n",
      "5827/5827 [==============================] - 5s 803us/step - loss: 0.3626 - accuracy: 0.8599 - f1_score: 0.8579 - val_loss: 0.4335 - val_accuracy: 0.8358 - val_f1_score: 0.8329\n"
     ]
    }
   ],
   "source": [
    "# Get wordToVector from [wordArr] and oneHotEncoding from [labelArr]\n",
    "wordToVecArr = WordToVector.getPretrainedWordToVecList(train_word_arr)\n",
    "oneHotEncodingArr = OneHotEncoder.getOneHotEncodingOfOutput(train_label_arr)\n",
    "# Convert python array to num py array\n",
    "np_wordToVecArr = np.array(wordToVecArr)\n",
    "np_oneHotEncodingArr = np.array(oneHotEncodingArr)\n",
    "print(\"Train set preprocess done\")\n",
    "\n",
    "# Get wordToVector from [wordArr] and oneHotEncoding from [labelArr]\n",
    "val_wordToVecArr = WordToVector.getPretrainedWordToVecList(val_word_arr)\n",
    "val_oneHotEncodingArr = OneHotEncoder.getOneHotEncodingOfOutput(val_label_arr)\n",
    "# Convert python array to num py array\n",
    "val_np_wordToVecArr = np.array(val_wordToVecArr)\n",
    "val_np_oneHotEncodingArr = np.array(val_oneHotEncodingArr)\n",
    "print(\"Validation set preprocess done\")\n",
    "\n",
    "training = DnnTraining(input_dim=300, output_dim=3)\n",
    "training.startTraining(np_wordToVecArr, np_oneHotEncodingArr, val_np_wordToVecArr, val_np_oneHotEncodingArr, epochs=100)\n",
    "training.saveTrainedModel()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras import backend as K\n",
    "\n",
    "model = training.getCurrentModel()\n",
    "\n",
    "# Get wordToVector from [TestWordArr] and oneHotEncoding from [TestLabelArr]\n",
    "test_wordToVecArr = WordToVector.getPretrainedWordToVecList(test_word_arr)\n",
    "test_oneHotEncodingArr = OneHotEncoder.getOneHotEncodingOfOutput(test_label_arr)\n",
    "# Convert python array to num py array\n",
    "test_np_wordToVecArr = np.array(test_wordToVecArr)\n",
    "test_np_oneHotEncodingArr = np.array(test_oneHotEncodingArr)\n",
    "\n",
    "# Assuming you have test data and labels\n",
    "x_test = test_np_wordToVecArr\n",
    "y_test = test_np_oneHotEncodingArr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833/833 [==============================] - 0s 376us/step\n",
      "Accuracy: 0.8392716350666416\n",
      "F1 score internal: 0.9547423389871676\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_test_argmax = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class labels (if needed)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "def internal_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom F1 score metric function.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    precision = true_positives / (predicted_positives)\n",
    "    recall = true_positives / (possible_positives)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test_argmax, y_pred_classes)\n",
    "f1_internal = internal_f1_score(y_test_argmax, y_pred_classes)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('F1 score internal:', f1_internal.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
